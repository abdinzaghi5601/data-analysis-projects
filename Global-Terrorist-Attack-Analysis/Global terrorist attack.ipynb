{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fbc51e1b-319f-4952-a32b-1d30271a14b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': None, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "Test Accuracy: 0.9309006852142326\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.76      0.54      0.63      3978\n",
      "           1       0.94      0.98      0.96     32361\n",
      "\n",
      "    accuracy                           0.93     36339\n",
      "   macro avg       0.85      0.76      0.80     36339\n",
      "weighted avg       0.92      0.93      0.93     36339\n",
      "\n",
      "\n",
      "Feature Importances:\n",
      "           Feature  Importance\n",
      "4    targtype1_txt    0.208826\n",
      "0            iyear    0.197534\n",
      "3  attacktype1_txt    0.190224\n",
      "1      country_txt    0.122186\n",
      "6            nkill    0.102945\n",
      "7           nwound    0.078312\n",
      "2       region_txt    0.054458\n",
      "5    weaptype1_txt    0.045516\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 1. Load the dataset\n",
    "# --------------------------------------------------------------------\n",
    "# Replace with your actual file path as needed\n",
    "df = pd.read_csv(\"globalterrorismdb_0718dist.csv\", encoding='latin-1', low_memory=False)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 2. Basic Data Cleaning\n",
    "# --------------------------------------------------------------------\n",
    "# Choose the columns you want for modeling. \n",
    "# Adjust this list based on your target and the features you find relevant.\n",
    "columns_needed = [\n",
    "    \"iyear\",            # Year\n",
    "    \"country_txt\",      # Country (categorical)\n",
    "    \"region_txt\",       # Region (categorical)\n",
    "    \"attacktype1_txt\",  # Attack type (categorical)\n",
    "    \"targtype1_txt\",    # Target type (categorical)\n",
    "    \"weaptype1_txt\",    # Weapon type (categorical)\n",
    "    \"nkill\",            # Number of people killed\n",
    "    \"nwound\",           # Number of people wounded\n",
    "    \"success\"           # Our target variable (1=successful, 0=unsuccessful)\n",
    "]\n",
    "\n",
    "# Keep only those columns (some might not exist in older versions of the dataset—adjust if needed)\n",
    "df = df[columns_needed].copy()\n",
    "\n",
    "# Example numeric cleaning: fill missing values with 0\n",
    "# (In practice, consider more nuanced imputation strategies)\n",
    "df[\"nkill\"] = df[\"nkill\"].fillna(0)\n",
    "df[\"nwound\"] = df[\"nwound\"].fillna(0)\n",
    "\n",
    "\n",
    "# Drop rows where 'success' is NaN (if any exist)\n",
    "df.dropna(subset=[\"success\"], inplace=True)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 3. Feature Engineering and Encoding\n",
    "# --------------------------------------------------------------------\n",
    "# Separate target (y) and features (X)\n",
    "y = df[\"success\"].astype(int)  # ensure it's int\n",
    "X = df.drop([\"success\"], axis=1)\n",
    "\n",
    "# Identify categorical columns to encode\n",
    "cat_cols = [\"country_txt\", \"region_txt\", \"attacktype1_txt\", \"targtype1_txt\", \"weaptype1_txt\"]\n",
    "# Numeric columns\n",
    "num_cols = [\"iyear\", \"nkill\", \"nwound\"]\n",
    "\n",
    "# Apply label encoding to the categorical columns\n",
    "encoder_map = {}\n",
    "for col in cat_cols:\n",
    "    encoder = LabelEncoder()\n",
    "    # Convert column to string before label-encoding to avoid errors\n",
    "    X[col] = encoder.fit_transform(X[col].astype(str))\n",
    "    # Store encoder if you need to transform new data later\n",
    "    encoder_map[col] = encoder\n",
    "\n",
    "# At this point, X has only numeric columns.\n",
    "# You could also do one-hot encoding (pd.get_dummies) for better performance on tree-based models.\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 4. Train/Test Split\n",
    "# --------------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, \n",
    "    y, \n",
    "    test_size=0.2,      # 20% for testing\n",
    "    random_state=42     # for reproducibility\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 5. Model Selection and Hyperparameter Tuning\n",
    "# --------------------------------------------------------------------\n",
    "# We'll use a Random Forest Classifier as an example\n",
    "rf = RandomForestClassifier(random_state=42)\n",
    "\n",
    "# A small parameter grid for demonstration. Feel free to expand it.\n",
    "param_grid = {\n",
    "    \"n_estimators\": [100, 200],\n",
    "    \"max_depth\": [None, 5, 10],\n",
    "    \"min_samples_split\": [2, 5]\n",
    "}\n",
    "\n",
    "# GridSearchCV will try each combination using cross-validation\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rf,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,              # 3-fold cross-validation\n",
    "    scoring=\"accuracy\",# You can switch to f1, recall, etc., if data is imbalanced\n",
    "    n_jobs=-1          # Use all CPU cores\n",
    ")\n",
    "\n",
    "# Train the models\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Retrieve the best model\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 6. Evaluation\n",
    "# --------------------------------------------------------------------\n",
    "# Predict on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "# Check best hyperparameters\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "\n",
    "# Compute accuracy\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(\"Test Accuracy:\", accuracy)\n",
    "\n",
    "# Classification report (precision, recall, F1-score)\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 7. Feature Importance (Optional)\n",
    "# --------------------------------------------------------------------\n",
    "# Feature importances can reveal which features contributed most to the model\n",
    "if hasattr(best_model, \"feature_importances_\"):\n",
    "    importances = best_model.feature_importances_\n",
    "    feature_names = X.columns\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        \"Feature\": feature_names,\n",
    "        \"Importance\": importances\n",
    "    }).sort_values(by=\"Importance\", ascending=False)\n",
    "    \n",
    "    print(\"\\nFeature Importances:\")\n",
    "    print(importance_df)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 8. Next Steps\n",
    "# --------------------------------------------------------------------\n",
    "# - Consider deeper data cleaning/imputation.\n",
    "# - Handle class imbalance if \"success\" is skewed.\n",
    "# - Try advanced techniques (XGBoost, LightGBM), or time-based splits \n",
    "#   if predicting future attacks based on older data.\n",
    "# - Incorporate domain knowledge for more relevant features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29d1def5-de80-4eaf-8406-447ccffbb44a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score: 0.9618985478658028\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Suppose you have ground truth labels and model predictions:\n",
    "# y_test = [...]\n",
    "# y_pred = [...]\n",
    "\n",
    "# Calculate the F1-score for a binary classification\n",
    "f1 = f1_score(y_test, y_pred)  # Default: pos_label=1\n",
    "\n",
    "print(\"F1-score:\", f1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e1831840-1fe0-431e-8cc8-b38a215ce3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1-score (macro): 0.79563562079637\n",
      "F1-score (weighted): 0.9254972201527559\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# y_test = [...]\n",
    "# y_pred = [...]\n",
    "\n",
    "# macro average – treats all classes equally\n",
    "f1_macro = f1_score(y_test, y_pred, average='macro')\n",
    "\n",
    "# weighted average – weights each class by its support (i.e., number of samples)\n",
    "f1_weighted = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "print(\"F1-score (macro):\", f1_macro)\n",
    "print(\"F1-score (weighted):\", f1_weighted)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c495137-ff62-410e-828d-90b7be7ed75c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters: {'max_depth': 10, 'min_samples_split': 2, 'n_estimators': 200}\n",
      "Test RMSE: 33.15702063861292\n",
      "Test R^2 Score: 0.6144554569296967\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 1. Load the dataset\n",
    "# --------------------------------------------------------------------\n",
    "df = pd.read_csv(\"globalterrorismdb_0718dist.csv\", encoding='latin-1', low_memory=False)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 2. Create 'casualties' target variable\n",
    "# --------------------------------------------------------------------\n",
    "# Fill missing values with 0 first, then sum\n",
    "df[\"nkill\"] = df[\"nkill\"].fillna(0)\n",
    "df[\"nwound\"] = df[\"nwound\"].fillna(0)\n",
    "\n",
    "df['casualties'] = df['nkill'] + df['nwound']\n",
    "\n",
    "# We'll drop rows where casualties is NaN just in case, but we already filled with 0\n",
    "df.dropna(subset=['casualties'], inplace=True)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 3. Select Features (X) and Target (y)\n",
    "# --------------------------------------------------------------------\n",
    "# For demonstration, let's use a few columns:\n",
    "#   - iyear (numeric)\n",
    "#   - attacktype1_txt (categorical)\n",
    "#   - weaptype1_txt (categorical)\n",
    "#   - targtype1_txt (categorical)\n",
    "#   - region_txt (categorical)\n",
    "#   - city (categorical) - optional, can be large cardinality\n",
    "\n",
    "columns_needed = [\n",
    "    'iyear',\n",
    "    'attacktype1_txt',\n",
    "    'weaptype1_txt',\n",
    "    'targtype1_txt',\n",
    "    'region_txt'\n",
    "]\n",
    "\n",
    "# Make sure these columns exist in the dataset\n",
    "df = df[columns_needed + ['casualties']].copy()\n",
    "\n",
    "# Define target\n",
    "y = df['casualties']\n",
    "X = df.drop('casualties', axis=1)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 4. Convert Categorical Columns to Numeric\n",
    "# --------------------------------------------------------------------\n",
    "cat_cols = ['attacktype1_txt', 'weaptype1_txt', 'targtype1_txt', 'region_txt']\n",
    "for col in cat_cols:\n",
    "    X[col] = X[col].astype(str)\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 5. Train/Test Split\n",
    "# --------------------------------------------------------------------\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 6. Model Definition & Hyperparameter Tuning\n",
    "# --------------------------------------------------------------------\n",
    "rfr = RandomForestRegressor(random_state=42)\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200],\n",
    "    'max_depth': [None, 10],\n",
    "    'min_samples_split': [2, 5]\n",
    "}\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=rfr,\n",
    "    param_grid=param_grid,\n",
    "    cv=3,\n",
    "    scoring='neg_mean_squared_error',\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "grid_search.fit(X_train, y_train)\n",
    "best_model = grid_search.best_estimator_\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# 7. Evaluation\n",
    "# --------------------------------------------------------------------\n",
    "y_pred = best_model.predict(X_test)\n",
    "\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Best Hyperparameters:\", grid_search.best_params_)\n",
    "print(\"Test RMSE:\", rmse)\n",
    "print(\"Test R^2 Score:\", r2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fedb4b8e-1be2-454c-8c64-cca424851a6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['iyear', 'attacktype1_txt', 'weaptype1_txt', 'targtype1_txt', 'region_txt', 'casualties']\n"
     ]
    }
   ],
   "source": [
    "print(df.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e8da65f-874b-4a86-86cc-1ac33eaf9d67",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
