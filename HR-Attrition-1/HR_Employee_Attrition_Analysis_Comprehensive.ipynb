{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# HR Employee Attrition Analysis - Comprehensive Study\n",
        "\n",
        "## Project Overview\n",
        "This comprehensive analysis explores employee attrition patterns using advanced machine learning and statistical techniques to identify key factors influencing employee turnover and develop predictive models for HR decision-making.\n",
        "\n",
        "**Key Objectives:**\n",
        "1. Identify primary drivers of employee attrition\n",
        "2. Build predictive models to identify at-risk employees\n",
        "3. Provide actionable insights for HR retention strategies\n",
        "4. Analyze demographic and job-related patterns\n",
        "\n",
        "**Dataset:** 1,470 employees with 35 features including demographics, job characteristics, compensation, and work environment factors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data manipulation and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
        "from sklearn.feature_selection import SelectKBest, chi2\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Set style for better visualizations\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "print(\"✅ Libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading and Initial Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset\n",
        "print(\"🔄 Loading HR Employee Attrition dataset...\")\n",
        "\n",
        "df = pd.read_csv('HR-Employee-Attrition.csv')\n",
        "\n",
        "print(f\"✅ Dataset loaded successfully!\")\n",
        "print(f\"📊 Dataset Shape: {df.shape}\")\n",
        "print(f\"👥 Total Employees: {len(df)}\")\n",
        "print(f\"📋 Features: {len(df.columns)}\")\n",
        "\n",
        "# Display basic information\n",
        "print(\"\\n=== DATASET OVERVIEW ===\")\n",
        "print(f\"Columns: {list(df.columns)}\")\n",
        "\n",
        "print(\"\\n=== DATA TYPES ===\")\n",
        "print(df.dtypes.value_counts())\n",
        "\n",
        "print(\"\\n=== MISSING VALUES ===\")\n",
        "missing_data = df.isnull().sum()\n",
        "if missing_data.sum() == 0:\n",
        "    print(\"✅ No missing values found!\")\n",
        "else:\n",
        "    print(missing_data[missing_data > 0])\n",
        "\n",
        "print(\"\\n=== ATTRITION DISTRIBUTION ===\")\n",
        "attrition_counts = df['Attrition'].value_counts()\n",
        "attrition_pct = df['Attrition'].value_counts(normalize=True) * 100\n",
        "print(f\"No Attrition: {attrition_counts['No']} ({attrition_pct['No']:.1f}%)\")\n",
        "print(f\"Attrition: {attrition_counts['Yes']} ({attrition_pct['Yes']:.1f}%)\")\n",
        "print(f\"Attrition Rate: {attrition_pct['Yes']:.1f}%\")\n",
        "\n",
        "# Display first few rows\n",
        "print(\"\\n=== SAMPLE DATA ===\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive statistical summary\n",
        "print(\"=== NUMERICAL FEATURES SUMMARY ===\")\n",
        "numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
        "print(f\"Numerical features ({len(numerical_features)}): {numerical_features}\")\n",
        "print(\"\\nStatistical Summary:\")\n",
        "display(df[numerical_features].describe())\n",
        "\n",
        "print(\"\\n=== CATEGORICAL FEATURES SUMMARY ===\")\n",
        "categorical_features = df.select_dtypes(include=['object']).columns.tolist()\n",
        "print(f\"Categorical features ({len(categorical_features)}): {categorical_features}\")\n",
        "print(\"\\nCategorical Summary:\")\n",
        "for col in categorical_features:\n",
        "    print(f\"\\n{col}: {df[col].nunique()} unique values\")\n",
        "    print(df[col].value_counts().head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Comprehensive Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a copy for preprocessing\n",
        "df_processed = df.copy()\n",
        "\n",
        "print(\"=== DATA PREPROCESSING ===\")\n",
        "\n",
        "# 1. Target variable encoding\n",
        "le = LabelEncoder()\n",
        "df_processed['Attrition_Target'] = le.fit_transform(df_processed['Attrition'])\n",
        "print(f\"✅ Target variable encoded: Yes=1, No=0\")\n",
        "\n",
        "# 2. Handle categorical variables\n",
        "# Binary categorical variables\n",
        "binary_cats = ['OverTime', 'Over18']\n",
        "for col in binary_cats:\n",
        "    if col in df_processed.columns:\n",
        "        df_processed[col + '_encoded'] = le.fit_transform(df_processed[col])\n",
        "\n",
        "# Multi-class categorical variables - One-hot encoding\n",
        "categorical_to_encode = ['BusinessTravel', 'Department', 'EducationField', 'Gender', \n",
        "                        'JobRole', 'MaritalStatus']\n",
        "df_encoded = pd.get_dummies(df_processed, columns=categorical_to_encode, drop_first=True)\n",
        "\n",
        "print(f\"✅ Categorical variables encoded\")\n",
        "print(f\"📊 New dataset shape: {df_encoded.shape}\")\n",
        "\n",
        "# 3. Feature engineering\n",
        "print(\"\\n🔧 Creating derived features...\")\n",
        "\n",
        "# Age groups\n",
        "df_encoded['AgeGroup'] = pd.cut(df_encoded['Age'], \n",
        "                               bins=[0, 30, 40, 50, 100], \n",
        "                               labels=['Under_30', '30-40', '40-50', 'Over_50'])\n",
        "df_encoded = pd.get_dummies(df_encoded, columns=['AgeGroup'], drop_first=True)\n",
        "\n",
        "# Income groups\n",
        "df_encoded['IncomeGroup'] = pd.cut(df_encoded['MonthlyIncome'], \n",
        "                                  bins=[0, 3000, 6000, 10000, 100000], \n",
        "                                  labels=['Low', 'Medium', 'High', 'Very_High'])\n",
        "df_encoded = pd.get_dummies(df_encoded, columns=['IncomeGroup'], drop_first=True)\n",
        "\n",
        "# Experience ratios\n",
        "df_encoded['ExperienceRatio'] = df_encoded['YearsAtCompany'] / (df_encoded['TotalWorkingYears'] + 1)\n",
        "df_encoded['PromotionRate'] = df_encoded['YearsSinceLastPromotion'] / (df_encoded['YearsAtCompany'] + 1)\n",
        "\n",
        "# Work-life balance score\n",
        "df_encoded['WorkLifeScore'] = (df_encoded['WorkLifeBalance'] + \n",
        "                              df_encoded['JobSatisfaction'] + \n",
        "                              df_encoded['EnvironmentSatisfaction']) / 3\n",
        "\n",
        "print(f\"✅ Feature engineering completed\")\n",
        "print(f\"📊 Final dataset shape: {df_encoded.shape}\")\n",
        "\n",
        "# Remove redundant columns\n",
        "columns_to_drop = ['Attrition', 'EmployeeCount', 'EmployeeNumber', 'Over18', 'StandardHours']\n",
        "df_final = df_encoded.drop(columns=[col for col in columns_to_drop if col in df_encoded.columns])\n",
        "\n",
        "print(f\"📊 Cleaned dataset shape: {df_final.shape}\")\n",
        "print(\"\\n✅ Preprocessing completed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive EDA with multiple visualizations\n",
        "print(\"=== EXPLORATORY DATA ANALYSIS ===\")\n",
        "\n",
        "# Create figure with multiple subplots\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# 1. Attrition distribution\n",
        "attrition_counts = df['Attrition'].value_counts()\n",
        "axes[0,0].pie(attrition_counts.values, labels=attrition_counts.index, autopct='%1.1f%%', \n",
        "              colors=['lightgreen', 'lightcoral'])\n",
        "axes[0,0].set_title('Attrition Distribution', fontsize=14, fontweight='bold')\n",
        "\n",
        "# 2. Age distribution by attrition\n",
        "df.boxplot(column='Age', by='Attrition', ax=axes[0,1])\n",
        "axes[0,1].set_title('Age Distribution by Attrition')\n",
        "axes[0,1].set_xlabel('Attrition')\n",
        "\n",
        "# 3. Monthly Income by attrition\n",
        "df.boxplot(column='MonthlyIncome', by='Attrition', ax=axes[0,2])\n",
        "axes[0,2].set_title('Monthly Income by Attrition')\n",
        "axes[0,2].set_xlabel('Attrition')\n",
        "\n",
        "# 4. Years at Company distribution\n",
        "df.boxplot(column='YearsAtCompany', by='Attrition', ax=axes[1,0])\n",
        "axes[1,0].set_title('Years at Company by Attrition')\n",
        "axes[1,0].set_xlabel('Attrition')\n",
        "\n",
        "# 5. Job Satisfaction by attrition\n",
        "df.boxplot(column='JobSatisfaction', by='Attrition', ax=axes[1,1])\n",
        "axes[1,1].set_title('Job Satisfaction by Attrition')\n",
        "axes[1,1].set_xlabel('Attrition')\n",
        "\n",
        "# 6. Distance from Home by attrition\n",
        "df.boxplot(column='DistanceFromHome', by='Attrition', ax=axes[1,2])\n",
        "axes[1,2].set_title('Distance from Home by Attrition')\n",
        "axes[1,2].set_xlabel('Attrition')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Statistical summaries by attrition\n",
        "print(\"\\n=== KEY METRICS BY ATTRITION STATUS ===\")\n",
        "key_metrics = ['Age', 'MonthlyIncome', 'YearsAtCompany', 'JobSatisfaction', 'DistanceFromHome']\n",
        "for metric in key_metrics:\n",
        "    no_attrition = df[df['Attrition'] == 'No'][metric].mean()\n",
        "    yes_attrition = df[df['Attrition'] == 'Yes'][metric].mean()\n",
        "    difference = no_attrition - yes_attrition\n",
        "    print(f\"{metric}:\")\n",
        "    print(f\"  No Attrition: {no_attrition:.2f}\")\n",
        "    print(f\"  Yes Attrition: {yes_attrition:.2f}\")\n",
        "    print(f\"  Difference: {difference:.2f}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Categorical features analysis\n",
        "print(\"=== CATEGORICAL FEATURES ANALYSIS ===\")\n",
        "\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "categorical_features = ['Department', 'JobRole', 'MaritalStatus', 'Gender', 'OverTime', 'BusinessTravel']\n",
        "\n",
        "for i, feature in enumerate(categorical_features):\n",
        "    row = i // 3\n",
        "    col = i % 3\n",
        "    \n",
        "    # Cross-tabulation\n",
        "    ct = pd.crosstab(df[feature], df['Attrition'], normalize='index')\n",
        "    ct.plot(kind='bar', ax=axes[row, col], color=['lightgreen', 'lightcoral'])\n",
        "    axes[row, col].set_title(f'Attrition Rate by {feature}')\n",
        "    axes[row, col].set_xlabel(feature)\n",
        "    axes[row, col].set_ylabel('Attrition Rate')\n",
        "    axes[row, col].tick_params(axis='x', rotation=45)\n",
        "    axes[row, col].legend(['No', 'Yes'])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Print attrition rates by category\n",
        "print(\"\\n=== ATTRITION RATES BY CATEGORY ===\")\n",
        "for feature in categorical_features:\n",
        "    print(f\"\\n{feature}:\")\n",
        "    attrition_rate = df.groupby(feature)['Attrition'].apply(lambda x: (x == 'Yes').sum() / len(x) * 100)\n",
        "    for category, rate in attrition_rate.items():\n",
        "        print(f\"  {category}: {rate:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Correlation Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive correlation analysis\n",
        "print(\"=== CORRELATION ANALYSIS ===\")\n",
        "\n",
        "# Select numerical features for correlation\n",
        "numerical_features = df_final.select_dtypes(include=[np.number]).columns.tolist()\n",
        "corr_matrix = df_final[numerical_features].corr()\n",
        "\n",
        "# Create correlation heatmap\n",
        "plt.figure(figsize=(16, 12))\n",
        "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "sns.heatmap(corr_matrix, mask=mask, annot=False, cmap='coolwarm', center=0,\n",
        "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
        "plt.title('Correlation Matrix - All Numerical Features', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Correlation with target variable\n",
        "target_corr = df_final.corr()['Attrition_Target'].abs().sort_values(ascending=False)\n",
        "print(\"\\n=== TOP 15 FEATURES CORRELATED WITH ATTRITION ===\")\n",
        "for i, (feature, corr) in enumerate(target_corr.head(16).items()):\n",
        "    if feature != 'Attrition_Target':\n",
        "        print(f\"{i:2d}. {feature:30s}: {corr:.3f}\")\n",
        "\n",
        "# Visualize top correlations\n",
        "plt.figure(figsize=(12, 8))\n",
        "top_features = target_corr.head(16)[1:]  # Exclude target itself\n",
        "plt.barh(range(len(top_features)), top_features.values, color='skyblue')\n",
        "plt.yticks(range(len(top_features)), top_features.index)\n",
        "plt.xlabel('Absolute Correlation with Attrition')\n",
        "plt.title('Top 15 Features Correlated with Attrition', fontsize=14, fontweight='bold')\n",
        "plt.gca().invert_yaxis()\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Advanced Machine Learning Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Prepare data for machine learning\n",
        "print(\"=== MACHINE LEARNING PIPELINE ===\")\n",
        "\n",
        "# Select features (exclude target and string columns)\n",
        "feature_columns = df_final.select_dtypes(include=[np.number]).columns.tolist()\n",
        "feature_columns.remove('Attrition_Target')\n",
        "\n",
        "X = df_final[feature_columns]\n",
        "y = df_final['Attrition_Target']\n",
        "\n",
        "print(f\"📊 Features selected: {len(feature_columns)}\")\n",
        "print(f\"📊 Samples: {len(X)}\")\n",
        "print(f\"📊 Target distribution: {y.value_counts().to_dict()}\")\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"📊 Training set: {X_train.shape}\")\n",
        "print(f\"📊 Test set: {X_test.shape}\")\n",
        "\n",
        "# Handle class imbalance with SMOTE\n",
        "smote = SMOTE(random_state=42)\n",
        "X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "X_train_scaled_balanced = scaler.fit_transform(X_train_balanced)\n",
        "\n",
        "print(f\"📊 Balanced training set: {X_train_balanced.shape}\")\n",
        "print(f\"📊 Balanced target distribution: {pd.Series(y_train_balanced).value_counts().to_dict()}\")\n",
        "\n",
        "print(\"\\n✅ Data preparation completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define and train multiple models\n",
        "print(\"=== MODEL TRAINING AND EVALUATION ===\")\n",
        "\n",
        "# Define models\n",
        "models = {\n",
        "    'Logistic Regression': {\n",
        "        'model': LogisticRegression(random_state=42, max_iter=1000),\n",
        "        'params': {\n",
        "            'C': [0.1, 1.0, 10.0],\n",
        "            'penalty': ['l1', 'l2'],\n",
        "            'solver': ['liblinear']\n",
        "        },\n",
        "        'scaled': True\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'model': RandomForestClassifier(random_state=42),\n",
        "        'params': {\n",
        "            'n_estimators': [100, 200],\n",
        "            'max_depth': [10, 20, None],\n",
        "            'min_samples_split': [2, 5],\n",
        "            'min_samples_leaf': [1, 2]\n",
        "        },\n",
        "        'scaled': False\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'model': GradientBoostingClassifier(random_state=42),\n",
        "        'params': {\n",
        "            'n_estimators': [100, 200],\n",
        "            'learning_rate': [0.05, 0.1, 0.2],\n",
        "            'max_depth': [3, 5, 7]\n",
        "        },\n",
        "        'scaled': False\n",
        "    },\n",
        "    'Support Vector Machine': {\n",
        "        'model': SVC(random_state=42, probability=True),\n",
        "        'params': {\n",
        "            'C': [0.1, 1.0, 10.0],\n",
        "            'gamma': ['scale', 'auto'],\n",
        "            'kernel': ['rbf', 'linear']\n",
        "        },\n",
        "        'scaled': True\n",
        "    }\n",
        "}\n",
        "\n",
        "# Train and evaluate models\n",
        "results = {}\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for name, model_config in models.items():\n",
        "    print(f\"\\n🤖 Training {name}...\")\n",
        "    \n",
        "    # Select appropriate data\n",
        "    if model_config['scaled']:\n",
        "        X_train_use = X_train_scaled_balanced\n",
        "        X_test_use = X_test_scaled\n",
        "    else:\n",
        "        X_train_use = X_train_balanced\n",
        "        X_test_use = X_test\n",
        "    \n",
        "    # Grid search with cross-validation\n",
        "    grid_search = GridSearchCV(\n",
        "        model_config['model'],\n",
        "        model_config['params'],\n",
        "        cv=cv,\n",
        "        scoring='roc_auc',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    grid_search.fit(X_train_use, y_train_balanced)\n",
        "    best_model = grid_search.best_estimator_\n",
        "    \n",
        "    # Cross-validation scores\n",
        "    cv_scores = cross_val_score(best_model, X_train_use, y_train_balanced, cv=cv, scoring='roc_auc')\n",
        "    \n",
        "    # Test predictions\n",
        "    y_pred = best_model.predict(X_test_use)\n",
        "    y_pred_proba = best_model.predict_proba(X_test_use)[:, 1]\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "    \n",
        "    results[name] = {\n",
        "        'model': best_model,\n",
        "        'best_params': grid_search.best_params_,\n",
        "        'cv_auc_mean': cv_scores.mean(),\n",
        "        'cv_auc_std': cv_scores.std(),\n",
        "        'test_accuracy': accuracy,\n",
        "        'test_auc': auc_score,\n",
        "        'predictions': y_pred,\n",
        "        'predictions_proba': y_pred_proba\n",
        "    }\n",
        "    \n",
        "    print(f\"  ✅ Best parameters: {grid_search.best_params_}\")\n",
        "    print(f\"  📊 CV AUC: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
        "    print(f\"  📊 Test Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"  📊 Test AUC: {auc_score:.4f}\")\n",
        "\n",
        "print(\"\\n✅ Model training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Model Evaluation and Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive model comparison\n",
        "print(\"=== MODEL COMPARISON ===\")\n",
        "\n",
        "# Create comparison DataFrame\n",
        "comparison_df = pd.DataFrame({\n",
        "    'Model': results.keys(),\n",
        "    'CV AUC': [results[name]['cv_auc_mean'] for name in results.keys()],\n",
        "    'CV AUC Std': [results[name]['cv_auc_std'] for name in results.keys()],\n",
        "    'Test Accuracy': [results[name]['test_accuracy'] for name in results.keys()],\n",
        "    'Test AUC': [results[name]['test_auc'] for name in results.keys()]\n",
        "})\n",
        "\n",
        "comparison_df = comparison_df.sort_values('Test AUC', ascending=False)\n",
        "print(\"\\n📊 Model Performance Summary:\")\n",
        "print(comparison_df.round(4))\n",
        "\n",
        "# Visualize model comparison\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 12))\n",
        "\n",
        "# AUC comparison\n",
        "model_names = list(results.keys())\n",
        "cv_aucs = [results[name]['cv_auc_mean'] for name in model_names]\n",
        "test_aucs = [results[name]['test_auc'] for name in model_names]\n",
        "\n",
        "x = np.arange(len(model_names))\n",
        "width = 0.35\n",
        "\n",
        "axes[0,0].bar(x - width/2, cv_aucs, width, label='CV AUC', alpha=0.8)\n",
        "axes[0,0].bar(x + width/2, test_aucs, width, label='Test AUC', alpha=0.8)\n",
        "axes[0,0].set_xlabel('Models')\n",
        "axes[0,0].set_ylabel('AUC Score')\n",
        "axes[0,0].set_title('Model Performance Comparison (AUC)')\n",
        "axes[0,0].set_xticks(x)\n",
        "axes[0,0].set_xticklabels(model_names, rotation=45, ha='right')\n",
        "axes[0,0].legend()\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Find best model\n",
        "best_model_name = comparison_df.iloc[0]['Model']\n",
        "best_model = results[best_model_name]['model']\n",
        "best_predictions = results[best_model_name]['predictions']\n",
        "\n",
        "print(f\"\\n🏆 Best Model: {best_model_name}\")\n",
        "print(f\"📊 Best Test AUC: {results[best_model_name]['test_auc']:.4f}\")\n",
        "print(f\"📊 Best Test Accuracy: {results[best_model_name]['test_accuracy']:.4f}\")\n",
        "\n",
        "# Confusion Matrix for best model\n",
        "cm = confusion_matrix(y_test, best_predictions)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[0,1])\n",
        "axes[0,1].set_title(f'Confusion Matrix - {best_model_name}')\n",
        "axes[0,1].set_ylabel('True Label')\n",
        "axes[0,1].set_xlabel('Predicted Label')\n",
        "\n",
        "# ROC Curves\n",
        "for name in results.keys():\n",
        "    fpr, tpr, _ = roc_curve(y_test, results[name]['predictions_proba'])\n",
        "    auc_score = results[name]['test_auc']\n",
        "    axes[1,0].plot(fpr, tpr, label=f'{name} (AUC = {auc_score:.3f})')\n",
        "\n",
        "axes[1,0].plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "axes[1,0].set_xlabel('False Positive Rate')\n",
        "axes[1,0].set_ylabel('True Positive Rate')\n",
        "axes[1,0].set_title('ROC Curves Comparison')\n",
        "axes[1,0].legend()\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Feature importance for best model (if available)\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': feature_columns,\n",
        "        'importance': best_model.feature_importances_\n",
        "    }).sort_values('importance', ascending=False).head(15)\n",
        "    \n",
        "    axes[1,1].barh(range(len(feature_importance)), feature_importance['importance'])\n",
        "    axes[1,1].set_yticks(range(len(feature_importance)))\n",
        "    axes[1,1].set_yticklabels(feature_importance['feature'])\n",
        "    axes[1,1].set_xlabel('Feature Importance')\n",
        "    axes[1,1].set_title(f'Top 15 Feature Importances - {best_model_name}')\n",
        "    axes[1,1].invert_yaxis()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Detailed classification report for best model\n",
        "print(f\"\\n📋 Detailed Classification Report - {best_model_name}:\")\n",
        "print(classification_report(y_test, best_predictions, target_names=['No Attrition', 'Attrition']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Feature Importance Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Detailed feature importance analysis\n",
        "print(\"=== FEATURE IMPORTANCE ANALYSIS ===\")\n",
        "\n",
        "if hasattr(best_model, 'feature_importances_'):\n",
        "    # Feature importance from best model\n",
        "    feature_importance_df = pd.DataFrame({\n",
        "        'Feature': feature_columns,\n",
        "        'Importance': best_model.feature_importances_\n",
        "    }).sort_values('Importance', ascending=False)\n",
        "    \n",
        "    print(f\"\\n🔍 Top 20 Most Important Features ({best_model_name}):\")\n",
        "    for i, (_, row) in enumerate(feature_importance_df.head(20).iterrows()):\n",
        "        print(f\"{i+1:2d}. {row['Feature']:30s}: {row['Importance']:.4f}\")\n",
        "    \n",
        "    # Visualize feature importance\n",
        "    plt.figure(figsize=(12, 10))\n",
        "    top_features = feature_importance_df.head(20)\n",
        "    plt.barh(range(len(top_features)), top_features['Importance'], color='skyblue')\n",
        "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
        "    plt.xlabel('Feature Importance')\n",
        "    plt.title(f'Top 20 Feature Importances - {best_model_name}', fontsize=14, fontweight='bold')\n",
        "    plt.gca().invert_yaxis()\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "    \n",
        "    # Cumulative importance\n",
        "    cumulative_importance = feature_importance_df['Importance'].cumsum()\n",
        "    n_features_80 = (cumulative_importance <= 0.8).sum() + 1\n",
        "    n_features_90 = (cumulative_importance <= 0.9).sum() + 1\n",
        "    \n",
        "    print(f\"\\n📊 Feature Selection Insights:\")\n",
        "    print(f\"   Features for 80% importance: {n_features_80}\")\n",
        "    print(f\"   Features for 90% importance: {n_features_90}\")\n",
        "    print(f\"   Total features: {len(feature_columns)}\")\n",
        "\n",
        "# Top features for business interpretation\n",
        "print(f\"\\n💼 Business Insights from Top Features:\")\n",
        "top_business_features = feature_importance_df.head(10)\n",
        "for _, row in top_business_features.iterrows():\n",
        "    feature = row['Feature']\n",
        "    importance = row['Importance']\n",
        "    \n",
        "    if 'OverTime' in feature:\n",
        "        print(f\"   🔸 {feature} ({importance:.3f}): Overtime work significantly impacts attrition\")\n",
        "    elif 'MonthlyIncome' in feature:\n",
        "        print(f\"   🔸 {feature} ({importance:.3f}): Compensation levels are crucial for retention\")\n",
        "    elif 'JobSatisfaction' in feature:\n",
        "        print(f\"   🔸 {feature} ({importance:.3f}): Job satisfaction directly affects turnover\")\n",
        "    elif 'Age' in feature:\n",
        "        print(f\"   🔸 {feature} ({importance:.3f}): Age demographics influence retention patterns\")\n",
        "    elif 'Years' in feature:\n",
        "        print(f\"   🔸 {feature} ({importance:.3f}): Experience and tenure are key retention factors\")\n",
        "    elif 'Distance' in feature:\n",
        "        print(f\"   🔸 {feature} ({importance:.3f}): Commute distance affects employee satisfaction\")\n",
        "    else:\n",
        "        print(f\"   🔸 {feature} ({importance:.3f}): Important factor for attrition prediction\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Business Insights and Recommendations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate comprehensive business insights\n",
        "print(\"=== BUSINESS INSIGHTS AND RECOMMENDATIONS ===\")\n",
        "\n",
        "# Calculate key statistics\n",
        "total_employees = len(df)\n",
        "attrition_count = len(df[df['Attrition'] == 'Yes'])\n",
        "attrition_rate = (attrition_count / total_employees) * 100\n",
        "\n",
        "print(f\"\\n📊 EXECUTIVE SUMMARY:\")\n",
        "print(f\"   Total Employees Analyzed: {total_employees:,}\")\n",
        "print(f\"   Employees with Attrition: {attrition_count:,}\")\n",
        "print(f\"   Overall Attrition Rate: {attrition_rate:.1f}%\")\n",
        "print(f\"   Best Predictive Model: {best_model_name}\")\n",
        "print(f\"   Model Accuracy: {results[best_model_name]['test_accuracy']:.1%}\")\n",
        "print(f\"   Model AUC Score: {results[best_model_name]['test_auc']:.3f}\")\n",
        "\n",
        "# Key risk factors analysis\n",
        "print(f\"\\n🚨 KEY RISK FACTORS IDENTIFIED:\")\n",
        "\n",
        "# Overtime analysis\n",
        "overtime_attrition = df[df['OverTime'] == 'Yes']['Attrition'].value_counts(normalize=True)['Yes'] * 100\n",
        "no_overtime_attrition = df[df['OverTime'] == 'No']['Attrition'].value_counts(normalize=True)['Yes'] * 100\n",
        "print(f\"   1. OVERTIME WORK:\")\n",
        "print(f\"      - Attrition rate with overtime: {overtime_attrition:.1f}%\")\n",
        "print(f\"      - Attrition rate without overtime: {no_overtime_attrition:.1f}%\")\n",
        "print(f\"      - Risk multiplier: {overtime_attrition/no_overtime_attrition:.1f}x higher\")\n",
        "\n",
        "# Age analysis\n",
        "young_employees = df[df['Age'] < 30]\n",
        "young_attrition = (young_employees['Attrition'] == 'Yes').mean() * 100\n",
        "mature_employees = df[df['Age'] >= 30]\n",
        "mature_attrition = (mature_employees['Attrition'] == 'Yes').mean() * 100\n",
        "print(f\"   2. AGE DEMOGRAPHICS:\")\n",
        "print(f\"      - Young employees (<30): {young_attrition:.1f}% attrition rate\")\n",
        "print(f\"      - Mature employees (30+): {mature_attrition:.1f}% attrition rate\")\n",
        "print(f\"      - Young employees are {young_attrition/mature_attrition:.1f}x more likely to leave\")\n",
        "\n",
        "# Income analysis\n",
        "low_income = df[df['MonthlyIncome'] < df['MonthlyIncome'].median()]\n",
        "high_income = df[df['MonthlyIncome'] >= df['MonthlyIncome'].median()]\n",
        "low_income_attrition = (low_income['Attrition'] == 'Yes').mean() * 100\n",
        "high_income_attrition = (high_income['Attrition'] == 'Yes').mean() * 100\n",
        "print(f\"   3. COMPENSATION LEVEL:\")\n",
        "print(f\"      - Below median income: {low_income_attrition:.1f}% attrition rate\")\n",
        "print(f\"      - Above median income: {high_income_attrition:.1f}% attrition rate\")\n",
        "print(f\"      - Lower income employees are {low_income_attrition/high_income_attrition:.1f}x more likely to leave\")\n",
        "\n",
        "# Department analysis\n",
        "print(f\"   4. DEPARTMENT-WISE RISK:\")\n",
        "dept_attrition = df.groupby('Department')['Attrition'].apply(lambda x: (x == 'Yes').mean() * 100).sort_values(ascending=False)\n",
        "for dept, rate in dept_attrition.items():\n",
        "    print(f\"      - {dept}: {rate:.1f}% attrition rate\")\n",
        "\n",
        "print(f\"\\n💡 STRATEGIC RECOMMENDATIONS:\")\n",
        "print(f\"   1. IMMEDIATE ACTIONS (0-3 months):\")\n",
        "print(f\"      🔸 Implement overtime monitoring and approval system\")\n",
        "print(f\"      🔸 Conduct exit interviews for departing employees\")\n",
        "print(f\"      🔸 Review compensation packages for below-median earners\")\n",
        "print(f\"      🔸 Deploy predictive model to identify at-risk employees\")\n",
        "\n",
        "print(f\"   2. SHORT-TERM INITIATIVES (3-6 months):\")\n",
        "print(f\"      🔸 Develop career development programs for young employees\")\n",
        "print(f\"      🔸 Implement flexible work arrangements to reduce overtime\")\n",
        "print(f\"      🔸 Create mentorship programs for early-career staff\")\n",
        "print(f\"      🔸 Enhance job satisfaction surveys and action plans\")\n",
        "\n",
        "print(f\"   3. LONG-TERM STRATEGY (6+ months):\")\n",
        "print(f\"      🔸 Redesign roles to improve work-life balance\")\n",
        "print(f\"      🔸 Implement performance-based retention bonuses\")\n",
        "print(f\"      🔸 Develop internal promotion pathways\")\n",
        "print(f\"      🔸 Create department-specific retention strategies\")\n",
        "\n",
        "print(f\"\\n📈 EXPECTED IMPACT:\")\n",
        "potential_reduction = 25  # Estimated percentage reduction in attrition\n",
        "potential_savings = attrition_count * potential_reduction / 100\n",
        "print(f\"   Potential attrition reduction: {potential_reduction}%\")\n",
        "print(f\"   Employees retained annually: ~{potential_savings:.0f}\")\n",
        "print(f\"   Estimated cost savings: Significant (recruitment, training, knowledge loss)\")\n",
        "\n",
        "print(f\"\\n🎯 SUCCESS METRICS:\")\n",
        "print(f\"   📊 Monthly attrition rate tracking\")\n",
        "print(f\"   📊 Overtime hours per employee\")\n",
        "print(f\"   📊 Job satisfaction survey scores\")\n",
        "print(f\"   📊 Early-career employee retention rate\")\n",
        "print(f\"   📊 Department-wise retention improvements\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"HR ATTRITION ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
        "print(\"This analysis provides data-driven insights for strategic HR decision-making.\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}