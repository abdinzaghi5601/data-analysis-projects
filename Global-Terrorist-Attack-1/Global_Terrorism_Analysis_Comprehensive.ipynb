{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Global Terrorism Database - Comprehensive Analysis\n",
        "\n",
        "## Project Overview\n",
        "This comprehensive analysis explores global terrorism patterns using the Global Terrorism Database (GTD) from 1970-2017. The project implements multiple machine learning approaches to understand and predict various aspects of terrorist activities.\n",
        "\n",
        "**Key Objectives:**\n",
        "1. **Attack Success Prediction**: Classify whether terrorist attacks will be successful\n",
        "2. **Casualty Prediction**: Predict the number of casualties (killed + wounded)\n",
        "3. **Attack Type Classification**: Predict the type of terrorist attack\n",
        "4. **Temporal Analysis**: Analyze trends and patterns over time\n",
        "5. **Geographical Analysis**: Identify regional patterns and hotspots\n",
        "6. **Risk Assessment**: Develop comprehensive risk scoring models\n",
        "\n",
        "**Dataset:** 181,691 terrorist incidents with 135+ features including location, attack details, casualties, and outcomes."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Import Libraries and Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Data manipulation and analysis\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import stats\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "try:\n",
        "    import plotly.express as px\n",
        "    import plotly.graph_objects as go\n",
        "    from plotly.subplots import make_subplots\n",
        "    PLOTLY_AVAILABLE = True\n",
        "except ImportError:\n",
        "    PLOTLY_AVAILABLE = False\n",
        "    print(\"Plotly not available, using matplotlib/seaborn only\")\n",
        "\n",
        "# Machine Learning\n",
        "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold, TimeSeriesSplit\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder, MinMaxScaler\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor, GradientBoostingClassifier, GradientBoostingRegressor\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression, Ridge, Lasso\n",
        "from sklearn.svm import SVC, SVR\n",
        "from sklearn.neural_network import MLPClassifier, MLPRegressor\n",
        "from sklearn.metrics import (\n",
        "    accuracy_score, classification_report, confusion_matrix, roc_auc_score, roc_curve,\n",
        "    mean_squared_error, mean_absolute_error, r2_score,\n",
        "    precision_recall_curve, f1_score\n",
        ")\n",
        "from sklearn.feature_selection import SelectKBest, chi2, f_regression\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "# Date/Time\n",
        "from datetime import datetime\n",
        "import calendar\n",
        "\n",
        "# Set style for visualizations\n",
        "plt.style.use('seaborn-v0_8')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "# Display settings\n",
        "pd.set_option('display.max_columns', None)\n",
        "pd.set_option('display.width', None)\n",
        "\n",
        "print(\"✅ Libraries imported successfully!\")\n",
        "print(f\"📊 Plotly available: {PLOTLY_AVAILABLE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Data Loading and Initial Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the Global Terrorism Database\n",
        "print(\"🔄 Loading Global Terrorism Database...\")\n",
        "\n",
        "df = pd.read_csv('globalterrorismdb_0718dist.csv', encoding='latin-1', low_memory=False)\n",
        "\n",
        "print(f\"✅ Dataset loaded successfully!\")\n",
        "print(f\"📊 Dataset Shape: {df.shape}\")\n",
        "print(f\"💣 Total Incidents: {len(df):,}\")\n",
        "print(f\"📋 Features: {len(df.columns)}\")\n",
        "print(f\"📅 Time Range: {df['iyear'].min()} - {df['iyear'].max()}\")\n",
        "\n",
        "# Display basic information\n",
        "print(\"\\n=== DATASET OVERVIEW ===\")\n",
        "print(f\"Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.1f} MB\")\n",
        "\n",
        "print(\"\\n=== KEY COLUMNS AVAILABLE ===\")\n",
        "key_columns = [\n",
        "    'iyear', 'imonth', 'iday', 'country_txt', 'region_txt', 'city',\n",
        "    'attacktype1_txt', 'targtype1_txt', 'weaptype1_txt', 'gname',\n",
        "    'nkill', 'nwound', 'success', 'suicide', 'property', 'propvalue'\n",
        "]\n",
        "\n",
        "for col in key_columns:\n",
        "    if col in df.columns:\n",
        "        print(f\"✅ {col}\")\n",
        "    else:\n",
        "        print(f\"❌ {col} (not available)\")\n",
        "\n",
        "print(\"\\n=== MISSING VALUES SUMMARY ===\")\n",
        "missing_summary = df[key_columns].isnull().sum().sort_values(ascending=False)\n",
        "missing_pct = (missing_summary / len(df) * 100).round(1)\n",
        "missing_df = pd.DataFrame({\n",
        "    'Missing_Count': missing_summary,\n",
        "    'Missing_Percentage': missing_pct\n",
        "})\n",
        "print(missing_df[missing_df['Missing_Count'] > 0])\n",
        "\n",
        "print(\"\\n=== SAMPLE DATA ===\")\n",
        "display(df[key_columns].head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive data exploration\n",
        "print(\"=== TEMPORAL DISTRIBUTION ===\")\n",
        "yearly_counts = df['iyear'].value_counts().sort_index()\n",
        "print(f\"Peak year: {yearly_counts.idxmax()} ({yearly_counts.max():,} incidents)\")\n",
        "print(f\"Lowest year: {yearly_counts.idxmin()} ({yearly_counts.min():,} incidents)\")\n",
        "print(f\"Average per year: {yearly_counts.mean():.0f} incidents\")\n",
        "\n",
        "print(\"\\n=== GEOGRAPHICAL DISTRIBUTION ===\")\n",
        "country_counts = df['country_txt'].value_counts().head(10)\n",
        "print(\"Top 10 most affected countries:\")\n",
        "for country, count in country_counts.items():\n",
        "    pct = (count / len(df)) * 100\n",
        "    print(f\"  {country}: {count:,} incidents ({pct:.1f}%)\")\n",
        "\n",
        "print(\"\\n=== ATTACK TYPES ===\")\n",
        "attack_types = df['attacktype1_txt'].value_counts().head(5)\n",
        "print(\"Top 5 attack types:\")\n",
        "for attack_type, count in attack_types.items():\n",
        "    pct = (count / len(df)) * 100\n",
        "    print(f\"  {attack_type}: {count:,} incidents ({pct:.1f}%)\")\n",
        "\n",
        "print(\"\\n=== CASUALTY STATISTICS ===\")\n",
        "total_killed = df['nkill'].sum()\n",
        "total_wounded = df['nwound'].sum()\n",
        "print(f\"Total killed: {total_killed:,}\")\n",
        "print(f\"Total wounded: {total_wounded:,}\")\n",
        "print(f\"Total casualties: {total_killed + total_wounded:,}\")\n",
        "print(f\"Average casualties per incident: {(total_killed + total_wounded) / len(df):.2f}\")\n",
        "\n",
        "print(\"\\n=== SUCCESS RATE ===\")\n",
        "success_rate = df['success'].mean() * 100\n",
        "print(f\"Overall success rate: {success_rate:.1f}%\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Comprehensive Data Preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create a comprehensive preprocessed dataset\n",
        "print(\"=== DATA PREPROCESSING ===\")\n",
        "\n",
        "# Create working copy\n",
        "df_processed = df.copy()\n",
        "\n",
        "# 1. Handle missing values systematically\n",
        "print(\"\\n🔧 Handling missing values...\")\n",
        "\n",
        "# Casualty data: fill with 0 (assumption: missing = no casualties)\n",
        "df_processed['nkill'] = df_processed['nkill'].fillna(0)\n",
        "df_processed['nwound'] = df_processed['nwound'].fillna(0)\n",
        "\n",
        "# Property value: fill with 0\n",
        "if 'propvalue' in df_processed.columns:\n",
        "    df_processed['propvalue'] = df_processed['propvalue'].fillna(0)\n",
        "\n",
        "# Text fields: fill with 'Unknown'\n",
        "text_cols = ['attacktype1_txt', 'targtype1_txt', 'weaptype1_txt', 'gname']\n",
        "for col in text_cols:\n",
        "    if col in df_processed.columns:\n",
        "        df_processed[col] = df_processed[col].fillna('Unknown')\n",
        "\n",
        "# Location fields: fill with 'Unknown'\n",
        "location_cols = ['country_txt', 'region_txt', 'city']\n",
        "for col in location_cols:\n",
        "    if col in df_processed.columns:\n",
        "        df_processed[col] = df_processed[col].fillna('Unknown')\n",
        "\n",
        "# 2. Feature engineering\n",
        "print(\"\\n🔧 Creating derived features...\")\n",
        "\n",
        "# Casualty features\n",
        "df_processed['total_casualties'] = df_processed['nkill'] + df_processed['nwound']\n",
        "df_processed['casualty_ratio'] = df_processed['nkill'] / (df_processed['total_casualties'] + 0.1)\n",
        "df_processed['high_casualty'] = (df_processed['total_casualties'] >= 10).astype(int)\n",
        "df_processed['mass_casualty'] = (df_processed['total_casualties'] >= 50).astype(int)\n",
        "\n",
        "# Temporal features\n",
        "df_processed['decade'] = (df_processed['iyear'] // 10) * 10\n",
        "df_processed['month_name'] = df_processed['imonth'].apply(lambda x: calendar.month_name[int(x)] if pd.notnull(x) and 1 <= x <= 12 else 'Unknown')\n",
        "df_processed['is_weekend'] = df_processed.apply(lambda row: \n",
        "    datetime(int(row['iyear']), int(row['imonth']) if pd.notnull(row['imonth']) else 1, \n",
        "             int(row['iday']) if pd.notnull(row['iday']) else 1).weekday() >= 5 \n",
        "    if pd.notnull(row['iyear']) else False, axis=1).astype(int)\n",
        "\n",
        "# Attack characteristics\n",
        "df_processed['is_suicide'] = df_processed.get('suicide', 0).fillna(0).astype(int)\n",
        "df_processed['has_property_damage'] = df_processed.get('property', 0).fillna(0).astype(int)\n",
        "\n",
        "# Geographic features\n",
        "df_processed['is_capital'] = df_processed['city'].str.contains('capital|Capital', na=False).astype(int)\n",
        "\n",
        "# Target variables for different prediction tasks\n",
        "df_processed['success_binary'] = df_processed['success'].fillna(0).astype(int)\n",
        "df_processed['casualty_category'] = pd.cut(df_processed['total_casualties'], \n",
        "                                          bins=[0, 1, 5, 15, 50, float('inf')], \n",
        "                                          labels=['None', 'Low', 'Medium', 'High', 'Extreme'])\n",
        "\n",
        "print(f\"✅ Feature engineering completed\")\n",
        "print(f\"📊 New dataset shape: {df_processed.shape}\")\n",
        "\n",
        "# 3. Create categorical encodings\n",
        "print(\"\\n🔧 Encoding categorical variables...\")\n",
        "\n",
        "# Select features for modeling\n",
        "categorical_features = [\n",
        "    'country_txt', 'region_txt', 'attacktype1_txt', 'targtype1_txt', 'weaptype1_txt'\n",
        "]\n",
        "\n",
        "numerical_features = [\n",
        "    'iyear', 'imonth', 'iday', 'nkill', 'nwound', 'total_casualties',\n",
        "    'casualty_ratio', 'high_casualty', 'mass_casualty', 'is_suicide',\n",
        "    'has_property_damage', 'is_weekend', 'is_capital'\n",
        "]\n",
        "\n",
        "# Create encoded version\n",
        "df_encoded = df_processed.copy()\n",
        "\n",
        "# Label encoding for categorical variables\n",
        "label_encoders = {}\n",
        "for col in categorical_features:\n",
        "    if col in df_encoded.columns:\n",
        "        le = LabelEncoder()\n",
        "        df_encoded[col + '_encoded'] = le.fit_transform(df_encoded[col].astype(str))\n",
        "        label_encoders[col] = le\n",
        "\n",
        "# One-hot encoding for selected categorical variables (for better tree model performance)\n",
        "high_cardinality_cols = ['country_txt', 'region_txt']\n",
        "low_cardinality_cols = ['attacktype1_txt', 'targtype1_txt', 'weaptype1_txt']\n",
        "\n",
        "for col in low_cardinality_cols:\n",
        "    if col in df_encoded.columns:\n",
        "        dummies = pd.get_dummies(df_encoded[col], prefix=col, drop_first=True)\n",
        "        df_encoded = pd.concat([df_encoded, dummies], axis=1)\n",
        "\n",
        "print(f\"✅ Categorical encoding completed\")\n",
        "print(f\"📊 Encoded dataset shape: {df_encoded.shape}\")\n",
        "\n",
        "print(\"\\n✅ Preprocessing completed successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Exploratory Data Analysis (EDA)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive EDA with multiple visualizations\n",
        "print(\"=== EXPLORATORY DATA ANALYSIS ===\")\n",
        "\n",
        "# Create comprehensive visualization\n",
        "fig, axes = plt.subplots(3, 3, figsize=(20, 18))\n",
        "\n",
        "# 1. Temporal trends\n",
        "yearly_incidents = df_processed.groupby('iyear').size()\n",
        "axes[0,0].plot(yearly_incidents.index, yearly_incidents.values, linewidth=2, color='red')\n",
        "axes[0,0].set_title('Terrorist Incidents Over Time', fontsize=14, fontweight='bold')\n",
        "axes[0,0].set_xlabel('Year')\n",
        "axes[0,0].set_ylabel('Number of Incidents')\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Regional distribution\n",
        "regional_counts = df_processed['region_txt'].value_counts().head(10)\n",
        "axes[0,1].barh(range(len(regional_counts)), regional_counts.values, color='skyblue')\n",
        "axes[0,1].set_yticks(range(len(regional_counts)))\n",
        "axes[0,1].set_yticklabels(regional_counts.index)\n",
        "axes[0,1].set_title('Incidents by Region (Top 10)', fontsize=14, fontweight='bold')\n",
        "axes[0,1].set_xlabel('Number of Incidents')\n",
        "\n",
        "# 3. Attack types\n",
        "attack_counts = df_processed['attacktype1_txt'].value_counts().head(8)\n",
        "colors = plt.cm.Set3(np.linspace(0, 1, len(attack_counts)))\n",
        "axes[0,2].pie(attack_counts.values, labels=attack_counts.index, autopct='%1.1f%%', colors=colors)\n",
        "axes[0,2].set_title('Attack Types Distribution', fontsize=14, fontweight='bold')\n",
        "\n",
        "# 4. Casualty distribution\n",
        "casualty_data = df_processed['total_casualties'][df_processed['total_casualties'] <= 100]\n",
        "axes[1,0].hist(casualty_data, bins=50, alpha=0.7, color='orange', edgecolor='black')\n",
        "axes[1,0].set_title('Total Casualties Distribution (≤100)', fontsize=14, fontweight='bold')\n",
        "axes[1,0].set_xlabel('Total Casualties')\n",
        "axes[1,0].set_ylabel('Frequency')\n",
        "axes[1,0].set_yscale('log')\n",
        "\n",
        "# 5. Success rate by attack type\n",
        "success_by_attack = df_processed.groupby('attacktype1_txt')['success_binary'].mean().sort_values(ascending=False).head(8)\n",
        "axes[1,1].bar(range(len(success_by_attack)), success_by_attack.values, color='green', alpha=0.7)\n",
        "axes[1,1].set_xticks(range(len(success_by_attack)))\n",
        "axes[1,1].set_xticklabels(success_by_attack.index, rotation=45, ha='right')\n",
        "axes[1,1].set_title('Success Rate by Attack Type', fontsize=14, fontweight='bold')\n",
        "axes[1,1].set_ylabel('Success Rate')\n",
        "\n",
        "# 6. Monthly patterns\n",
        "monthly_incidents = df_processed.groupby('imonth').size()\n",
        "month_names = [calendar.month_abbr[i] for i in range(1, 13)]\n",
        "axes[1,2].bar(range(1, 13), [monthly_incidents.get(i, 0) for i in range(1, 13)], color='purple', alpha=0.7)\n",
        "axes[1,2].set_xticks(range(1, 13))\n",
        "axes[1,2].set_xticklabels(month_names)\n",
        "axes[1,2].set_title('Incidents by Month', fontsize=14, fontweight='bold')\n",
        "axes[1,2].set_ylabel('Number of Incidents')\n",
        "\n",
        "# 7. Weapon types\n",
        "weapon_counts = df_processed['weaptype1_txt'].value_counts().head(6)\n",
        "axes[2,0].barh(range(len(weapon_counts)), weapon_counts.values, color='red', alpha=0.7)\n",
        "axes[2,0].set_yticks(range(len(weapon_counts)))\n",
        "axes[2,0].set_yticklabels(weapon_counts.index)\n",
        "axes[2,0].set_title('Primary Weapon Types', fontsize=14, fontweight='bold')\n",
        "axes[2,0].set_xlabel('Number of Incidents')\n",
        "\n",
        "# 8. Target types\n",
        "target_counts = df_processed['targtype1_txt'].value_counts().head(8)\n",
        "axes[2,1].bar(range(len(target_counts)), target_counts.values, color='brown', alpha=0.7)\n",
        "axes[2,1].set_xticks(range(len(target_counts)))\n",
        "axes[2,1].set_xticklabels(target_counts.index, rotation=45, ha='right')\n",
        "axes[2,1].set_title('Primary Target Types', fontsize=14, fontweight='bold')\n",
        "axes[2,1].set_ylabel('Number of Incidents')\n",
        "\n",
        "# 9. Decade comparison\n",
        "decade_casualties = df_processed.groupby('decade')['total_casualties'].sum()\n",
        "axes[2,2].bar(decade_casualties.index, decade_casualties.values, color='darkblue', alpha=0.7)\n",
        "axes[2,2].set_title('Total Casualties by Decade', fontsize=14, fontweight='bold')\n",
        "axes[2,2].set_xlabel('Decade')\n",
        "axes[2,2].set_ylabel('Total Casualties')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Summary statistics\n",
        "print(\"\\n=== KEY INSIGHTS ===\")\n",
        "print(f\"📈 Peak incident year: {yearly_incidents.idxmax()} ({yearly_incidents.max():,} incidents)\")\n",
        "print(f\"🌍 Most affected region: {regional_counts.index[0]} ({regional_counts.iloc[0]:,} incidents)\")\n",
        "print(f\"💥 Most common attack: {attack_counts.index[0]} ({attack_counts.iloc[0]:,} incidents)\")\n",
        "print(f\"🎯 Most targeted: {target_counts.index[0]} ({target_counts.iloc[0]:,} incidents)\")\n",
        "print(f\"🔫 Most used weapon: {weapon_counts.index[0]} ({weapon_counts.iloc[0]:,} incidents)\")\n",
        "print(f\"📊 Overall success rate: {df_processed['success_binary'].mean()*100:.1f}%\")\n",
        "print(f\"💀 Average casualties per incident: {df_processed['total_casualties'].mean():.2f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Advanced Statistical Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Advanced statistical analysis\n",
        "print(\"=== STATISTICAL ANALYSIS ===\")\n",
        "\n",
        "# 1. Correlation analysis\n",
        "print(\"\\n📊 Correlation Analysis:\")\n",
        "correlation_features = [\n",
        "    'iyear', 'total_casualties', 'nkill', 'nwound', 'success_binary',\n",
        "    'high_casualty', 'mass_casualty', 'is_suicide', 'has_property_damage'\n",
        "]\n",
        "\n",
        "corr_matrix = df_processed[correlation_features].corr()\n",
        "\n",
        "plt.figure(figsize=(12, 10))\n",
        "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
        "sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdYlBu_r', center=0,\n",
        "            square=True, linewidths=0.5, cbar_kws={\"shrink\": .8})\n",
        "plt.title('Correlation Matrix - Key Variables', fontsize=16, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 2. Success rate analysis by various factors\n",
        "print(\"\\n📊 Success Rate Analysis:\")\n",
        "\n",
        "factors = {\n",
        "    'Attack Type': 'attacktype1_txt',\n",
        "    'Target Type': 'targtype1_txt',\n",
        "    'Weapon Type': 'weaptype1_txt',\n",
        "    'Region': 'region_txt'\n",
        "}\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
        "axes = axes.ravel()\n",
        "\n",
        "for i, (factor_name, factor_col) in enumerate(factors.items()):\n",
        "    success_by_factor = df_processed.groupby(factor_col)['success_binary'].agg(['mean', 'count'])\n",
        "    success_by_factor = success_by_factor[success_by_factor['count'] >= 100].sort_values('mean', ascending=False).head(10)\n",
        "    \n",
        "    bars = axes[i].bar(range(len(success_by_factor)), success_by_factor['mean'], \n",
        "                      color=plt.cm.viridis(np.linspace(0, 1, len(success_by_factor))))\n",
        "    axes[i].set_xticks(range(len(success_by_factor)))\n",
        "    axes[i].set_xticklabels(success_by_factor.index, rotation=45, ha='right')\n",
        "    axes[i].set_title(f'Success Rate by {factor_name}', fontsize=12, fontweight='bold')\n",
        "    axes[i].set_ylabel('Success Rate')\n",
        "    \n",
        "    # Add value labels on bars\n",
        "    for bar, value in zip(bars, success_by_factor['mean']):\n",
        "        axes[i].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.01,\n",
        "                    f'{value:.2f}', ha='center', va='bottom', fontsize=8)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# 3. Casualty analysis\n",
        "print(\"\\n📊 Casualty Analysis:\")\n",
        "\n",
        "casualty_by_attack = df_processed.groupby('attacktype1_txt')['total_casualties'].agg(['mean', 'median', 'std', 'count'])\n",
        "casualty_by_attack = casualty_by_attack[casualty_by_attack['count'] >= 100].sort_values('mean', ascending=False).head(10)\n",
        "\n",
        "print(\"Average casualties by attack type (top 10):\")\n",
        "for attack_type, stats in casualty_by_attack.iterrows():\n",
        "    print(f\"  {attack_type}: {stats['mean']:.2f} avg, {stats['median']:.1f} median ({stats['count']} incidents)\")\n",
        "\n",
        "# 4. Temporal trends analysis\n",
        "print(\"\\n📊 Temporal Trends:\")\n",
        "\n",
        "yearly_stats = df_processed.groupby('iyear').agg({\n",
        "    'eventid': 'count',\n",
        "    'total_casualties': ['sum', 'mean'],\n",
        "    'success_binary': 'mean'\n",
        "}).round(2)\n",
        "\n",
        "yearly_stats.columns = ['incidents', 'total_casualties', 'avg_casualties', 'success_rate']\n",
        "\n",
        "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
        "\n",
        "# Incidents over time\n",
        "axes[0,0].plot(yearly_stats.index, yearly_stats['incidents'], linewidth=2, color='blue')\n",
        "axes[0,0].set_title('Annual Incidents', fontweight='bold')\n",
        "axes[0,0].set_ylabel('Number of Incidents')\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Total casualties over time\n",
        "axes[0,1].plot(yearly_stats.index, yearly_stats['total_casualties'], linewidth=2, color='red')\n",
        "axes[0,1].set_title('Annual Total Casualties', fontweight='bold')\n",
        "axes[0,1].set_ylabel('Total Casualties')\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# Average casualties per incident\n",
        "axes[1,0].plot(yearly_stats.index, yearly_stats['avg_casualties'], linewidth=2, color='orange')\n",
        "axes[1,0].set_title('Average Casualties per Incident', fontweight='bold')\n",
        "axes[1,0].set_ylabel('Average Casualties')\n",
        "axes[1,0].set_xlabel('Year')\n",
        "axes[1,0].grid(True, alpha=0.3)\n",
        "\n",
        "# Success rate over time\n",
        "axes[1,1].plot(yearly_stats.index, yearly_stats['success_rate'], linewidth=2, color='green')\n",
        "axes[1,1].set_title('Annual Success Rate', fontweight='bold')\n",
        "axes[1,1].set_ylabel('Success Rate')\n",
        "axes[1,1].set_xlabel('Year')\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\n📈 Trend Analysis:\")\n",
        "print(f\"   Peak incidents: {yearly_stats['incidents'].max():,} in {yearly_stats['incidents'].idxmax()}\")\n",
        "print(f\"   Peak casualties: {yearly_stats['total_casualties'].max():,} in {yearly_stats['total_casualties'].idxmax()}\")\n",
        "print(f\"   Highest avg casualties: {yearly_stats['avg_casualties'].max():.2f} in {yearly_stats['avg_casualties'].idxmax()}\")\n",
        "print(f\"   Recent trends (2010-2017):\")\n",
        "recent_trend = yearly_stats.loc[2010:2017]\n",
        "print(f\"     Avg incidents/year: {recent_trend['incidents'].mean():.0f}\")\n",
        "print(f\"     Avg casualties/year: {recent_trend['total_casualties'].mean():.0f}\")\n",
        "print(f\"     Avg success rate: {recent_trend['success_rate'].mean():.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Machine Learning Pipeline - Task 1: Attack Success Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 1: Predict attack success (binary classification)\n",
        "print(\"=== TASK 1: ATTACK SUCCESS PREDICTION ===\")\n",
        "\n",
        "# Prepare data for classification\n",
        "print(\"\\n🔧 Preparing data for classification...\")\n",
        "\n",
        "# Select features for attack success prediction\n",
        "feature_columns = [\n",
        "    'iyear', 'imonth', 'nkill', 'nwound', 'total_casualties',\n",
        "    'is_suicide', 'has_property_damage', 'is_weekend',\n",
        "    'country_txt_encoded', 'region_txt_encoded', 'attacktype1_txt_encoded',\n",
        "    'targtype1_txt_encoded', 'weaptype1_txt_encoded'\n",
        "]\n",
        "\n",
        "# Filter available columns\n",
        "available_features = [col for col in feature_columns if col in df_encoded.columns]\n",
        "print(f\"Available features: {len(available_features)}\")\n",
        "\n",
        "# Prepare data\n",
        "X = df_encoded[available_features].copy()\n",
        "y = df_encoded['success_binary'].copy()\n",
        "\n",
        "# Remove rows with missing target\n",
        "mask = ~y.isnull()\n",
        "X = X[mask]\n",
        "y = y[mask]\n",
        "\n",
        "# Handle any remaining missing values\n",
        "X = X.fillna(0)\n",
        "\n",
        "print(f\"📊 Dataset shape: {X.shape}\")\n",
        "print(f\"📊 Target distribution: {y.value_counts().to_dict()}\")\n",
        "print(f\"📊 Success rate: {y.mean()*100:.1f}%\")\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "print(f\"📊 Training set: {X_train.shape}\")\n",
        "print(f\"📊 Test set: {X_test.shape}\")\n",
        "\n",
        "# Handle class imbalance with SMOTE (if needed)\n",
        "if y.value_counts().min() / len(y) < 0.1:  # If minority class < 10%\n",
        "    print(\"\\n🔧 Applying SMOTE for class balancing...\")\n",
        "    smote = SMOTE(random_state=42)\n",
        "    X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)\n",
        "    X_train_scaled_balanced = scaler.fit_transform(X_train_balanced)\n",
        "    \n",
        "    print(f\"📊 Balanced training set: {X_train_balanced.shape}\")\n",
        "    print(f\"📊 Balanced target distribution: {pd.Series(y_train_balanced).value_counts().to_dict()}\")\n",
        "else:\n",
        "    X_train_balanced = X_train\n",
        "    y_train_balanced = y_train\n",
        "    X_train_scaled_balanced = X_train_scaled\n",
        "\n",
        "print(\"\\n✅ Data preparation completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train multiple models for attack success prediction\n",
        "print(\"=== MODEL TRAINING: ATTACK SUCCESS PREDICTION ===\")\n",
        "\n",
        "# Define models\n",
        "classification_models = {\n",
        "    'Logistic Regression': {\n",
        "        'model': LogisticRegression(random_state=42, max_iter=1000),\n",
        "        'params': {\n",
        "            'C': [0.1, 1.0, 10.0],\n",
        "            'penalty': ['l1', 'l2'],\n",
        "            'solver': ['liblinear']\n",
        "        },\n",
        "        'scaled': True\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'model': RandomForestClassifier(random_state=42),\n",
        "        'params': {\n",
        "            'n_estimators': [100, 200],\n",
        "            'max_depth': [10, 20, None],\n",
        "            'min_samples_split': [2, 5],\n",
        "            'min_samples_leaf': [1, 2]\n",
        "        },\n",
        "        'scaled': False\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'model': GradientBoostingClassifier(random_state=42),\n",
        "        'params': {\n",
        "            'n_estimators': [100, 200],\n",
        "            'learning_rate': [0.05, 0.1, 0.2],\n",
        "            'max_depth': [3, 5, 7]\n",
        "        },\n",
        "        'scaled': False\n",
        "    },\n",
        "    'Neural Network': {\n",
        "        'model': MLPClassifier(random_state=42, max_iter=500),\n",
        "        'params': {\n",
        "            'hidden_layer_sizes': [(50,), (100,), (50, 50)],\n",
        "            'alpha': [0.001, 0.01, 0.1],\n",
        "            'learning_rate_init': [0.001, 0.01]\n",
        "        },\n",
        "        'scaled': True\n",
        "    }\n",
        "}\n",
        "\n",
        "# Train models\n",
        "classification_results = {}\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for name, model_config in classification_models.items():\n",
        "    print(f\"\\n🤖 Training {name}...\")\n",
        "    \n",
        "    # Select appropriate data\n",
        "    if model_config['scaled']:\n",
        "        X_train_use = X_train_scaled_balanced\n",
        "        X_test_use = X_test_scaled\n",
        "    else:\n",
        "        X_train_use = X_train_balanced\n",
        "        X_test_use = X_test\n",
        "    \n",
        "    # Grid search\n",
        "    grid_search = GridSearchCV(\n",
        "        model_config['model'],\n",
        "        model_config['params'],\n",
        "        cv=cv,\n",
        "        scoring='roc_auc',\n",
        "        n_jobs=-1\n",
        "    )\n",
        "    \n",
        "    grid_search.fit(X_train_use, y_train_balanced)\n",
        "    best_model = grid_search.best_estimator_\n",
        "    \n",
        "    # Cross-validation scores\n",
        "    cv_scores = cross_val_score(best_model, X_train_use, y_train_balanced, cv=cv, scoring='roc_auc')\n",
        "    \n",
        "    # Test predictions\n",
        "    y_pred = best_model.predict(X_test_use)\n",
        "    y_pred_proba = best_model.predict_proba(X_test_use)[:, 1]\n",
        "    \n",
        "    # Calculate metrics\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    auc_score = roc_auc_score(y_test, y_pred_proba)\n",
        "    f1 = f1_score(y_test, y_pred)\n",
        "    \n",
        "    classification_results[name] = {\n",
        "        'model': best_model,\n",
        "        'best_params': grid_search.best_params_,\n",
        "        'cv_auc_mean': cv_scores.mean(),\n",
        "        'cv_auc_std': cv_scores.std(),\n",
        "        'test_accuracy': accuracy,\n",
        "        'test_auc': auc_score,\n",
        "        'test_f1': f1,\n",
        "        'predictions': y_pred,\n",
        "        'predictions_proba': y_pred_proba\n",
        "    }\n",
        "    \n",
        "    print(f\"  ✅ Best parameters: {grid_search.best_params_}\")\n",
        "    print(f\"  📊 CV AUC: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
        "    print(f\"  📊 Test Accuracy: {accuracy:.4f}\")\n",
        "    print(f\"  📊 Test AUC: {auc_score:.4f}\")\n",
        "    print(f\"  📊 Test F1: {f1:.4f}\")\n",
        "\n",
        "print(\"\\n✅ Classification model training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Machine Learning Pipeline - Task 2: Casualty Prediction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Task 2: Predict casualties (regression)\n",
        "print(\"=== TASK 2: CASUALTY PREDICTION ===\")\n",
        "\n",
        "# Prepare data for regression\n",
        "print(\"\\n🔧 Preparing data for casualty prediction...\")\n",
        "\n",
        "# Use same features but exclude casualty-related targets\n",
        "regression_features = [\n",
        "    'iyear', 'imonth', 'is_suicide', 'has_property_damage', 'is_weekend',\n",
        "    'country_txt_encoded', 'region_txt_encoded', 'attacktype1_txt_encoded',\n",
        "    'targtype1_txt_encoded', 'weaptype1_txt_encoded'\n",
        "]\n",
        "\n",
        "available_reg_features = [col for col in regression_features if col in df_encoded.columns]\n",
        "\n",
        "X_reg = df_encoded[available_reg_features].copy()\n",
        "y_reg = df_encoded['total_casualties'].copy()\n",
        "\n",
        "# Remove outliers (casualties > 1000) for better model performance\n",
        "mask = (y_reg <= 1000) & (~y_reg.isnull())\n",
        "X_reg = X_reg[mask]\n",
        "y_reg = y_reg[mask]\n",
        "\n",
        "# Handle missing values\n",
        "X_reg = X_reg.fillna(0)\n",
        "\n",
        "print(f\"📊 Regression dataset shape: {X_reg.shape}\")\n",
        "print(f\"📊 Casualty statistics:\")\n",
        "print(f\"   Mean: {y_reg.mean():.2f}\")\n",
        "print(f\"   Median: {y_reg.median():.1f}\")\n",
        "print(f\"   Max: {y_reg.max():.0f}\")\n",
        "print(f\"   % Zero casualties: {(y_reg == 0).mean()*100:.1f}%\")\n",
        "\n",
        "# Split data\n",
        "X_reg_train, X_reg_test, y_reg_train, y_reg_test = train_test_split(\n",
        "    X_reg, y_reg, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Scale features\n",
        "scaler_reg = StandardScaler()\n",
        "X_reg_train_scaled = scaler_reg.fit_transform(X_reg_train)\n",
        "X_reg_test_scaled = scaler_reg.transform(X_reg_test)\n",
        "\n",
        "print(f\"📊 Regression training set: {X_reg_train.shape}\")\n",
        "print(f\"📊 Regression test set: {X_reg_test.shape}\")\n",
        "\n",
        "print(\"\\n✅ Regression data preparation completed!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train regression models for casualty prediction\n",
        "print(\"=== MODEL TRAINING: CASUALTY PREDICTION ===\")\n",
        "\n",
        "# Define regression models\n",
        "regression_models = {\n",
        "    'Linear Regression': {\n",
        "        'model': LinearRegression(),\n",
        "        'params': {},\n",
        "        'scaled': True\n",
        "    },\n",
        "    'Ridge Regression': {\n",
        "        'model': Ridge(random_state=42),\n",
        "        'params': {\n",
        "            'alpha': [0.1, 1.0, 10.0, 100.0]\n",
        "        },\n",
        "        'scaled': True\n",
        "    },\n",
        "    'Random Forest': {\n",
        "        'model': RandomForestRegressor(random_state=42),\n",
        "        'params': {\n",
        "            'n_estimators': [100, 200],\n",
        "            'max_depth': [10, 20, None],\n",
        "            'min_samples_split': [2, 5],\n",
        "            'min_samples_leaf': [1, 2]\n",
        "        },\n",
        "        'scaled': False\n",
        "    },\n",
        "    'Gradient Boosting': {\n",
        "        'model': GradientBoostingRegressor(random_state=42),\n",
        "        'params': {\n",
        "            'n_estimators': [100, 200],\n",
        "            'learning_rate': [0.05, 0.1, 0.2],\n",
        "            'max_depth': [3, 5, 7]\n",
        "        },\n",
        "        'scaled': False\n",
        "    }\n",
        "}\n",
        "\n",
        "# Train regression models\n",
        "regression_results = {}\n",
        "cv_reg = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "for name, model_config in regression_models.items():\n",
        "    print(f\"\\n🤖 Training {name}...\")\n",
        "    \n",
        "    # Select appropriate data\n",
        "    if model_config['scaled']:\n",
        "        X_train_use = X_reg_train_scaled\n",
        "        X_test_use = X_reg_test_scaled\n",
        "    else:\n",
        "        X_train_use = X_reg_train\n",
        "        X_test_use = X_reg_test\n",
        "    \n",
        "    # Grid search or direct training\n",
        "    if model_config['params']:\n",
        "        grid_search = GridSearchCV(\n",
        "            model_config['model'],\n",
        "            model_config['params'],\n",
        "            cv=cv_reg,\n",
        "            scoring='r2',\n",
        "            n_jobs=-1\n",
        "        )\n",
        "        grid_search.fit(X_train_use, y_reg_train)\n",
        "        best_model = grid_search.best_estimator_\n",
        "        best_params = grid_search.best_params_\n",
        "    else:\n",
        "        best_model = model_config['model']\n",
        "        best_model.fit(X_train_use, y_reg_train)\n",
        "        best_params = {}\n",
        "    \n",
        "    # Cross-validation scores\n",
        "    cv_scores = cross_val_score(best_model, X_train_use, y_reg_train, cv=cv_reg, scoring='r2')\n",
        "    \n",
        "    # Test predictions\n",
        "    y_reg_pred = best_model.predict(X_test_use)\n",
        "    \n",
        "    # Calculate metrics\n",
        "    mae = mean_absolute_error(y_reg_test, y_reg_pred)\n",
        "    mse = mean_squared_error(y_reg_test, y_reg_pred)\n",
        "    rmse = np.sqrt(mse)\n",
        "    r2 = r2_score(y_reg_test, y_reg_pred)\n",
        "    \n",
        "    regression_results[name] = {\n",
        "        'model': best_model,\n",
        "        'best_params': best_params,\n",
        "        'cv_r2_mean': cv_scores.mean(),\n",
        "        'cv_r2_std': cv_scores.std(),\n",
        "        'test_mae': mae,\n",
        "        'test_rmse': rmse,\n",
        "        'test_r2': r2,\n",
        "        'predictions': y_reg_pred\n",
        "    }\n",
        "    \n",
        "    print(f\"  ✅ Best parameters: {best_params}\")\n",
        "    print(f\"  📊 CV R²: {cv_scores.mean():.4f} ± {cv_scores.std():.4f}\")\n",
        "    print(f\"  📊 Test R²: {r2:.4f}\")\n",
        "    print(f\"  📊 Test MAE: {mae:.4f}\")\n",
        "    print(f\"  📊 Test RMSE: {rmse:.4f}\")\n",
        "\n",
        "print(\"\\n✅ Regression model training completed!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Model Evaluation and Comparison"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive model evaluation and comparison\n",
        "print(\"=== MODEL EVALUATION AND COMPARISON ===\")\n",
        "\n",
        "# Classification models comparison\n",
        "print(\"\\n🎯 Classification Models (Attack Success Prediction):\")\n",
        "classification_comparison = pd.DataFrame({\n",
        "    'Model': classification_results.keys(),\n",
        "    'CV AUC': [classification_results[name]['cv_auc_mean'] for name in classification_results.keys()],\n",
        "    'CV AUC Std': [classification_results[name]['cv_auc_std'] for name in classification_results.keys()],\n",
        "    'Test Accuracy': [classification_results[name]['test_accuracy'] for name in classification_results.keys()],\n",
        "    'Test AUC': [classification_results[name]['test_auc'] for name in classification_results.keys()],\n",
        "    'Test F1': [classification_results[name]['test_f1'] for name in classification_results.keys()]\n",
        "})\n",
        "\n",
        "classification_comparison = classification_comparison.sort_values('Test AUC', ascending=False)\n",
        "print(classification_comparison.round(4))\n",
        "\n",
        "# Regression models comparison\n",
        "print(\"\\n🎯 Regression Models (Casualty Prediction):\")\n",
        "regression_comparison = pd.DataFrame({\n",
        "    'Model': regression_results.keys(),\n",
        "    'CV R²': [regression_results[name]['cv_r2_mean'] for name in regression_results.keys()],\n",
        "    'CV R² Std': [regression_results[name]['cv_r2_std'] for name in regression_results.keys()],\n",
        "    'Test R²': [regression_results[name]['test_r2'] for name in regression_results.keys()],\n",
        "    'Test MAE': [regression_results[name]['test_mae'] for name in regression_results.keys()],\n",
        "    'Test RMSE': [regression_results[name]['test_rmse'] for name in regression_results.keys()]\n",
        "})\n",
        "\n",
        "regression_comparison = regression_comparison.sort_values('Test R²', ascending=False)\n",
        "print(regression_comparison.round(4))\n",
        "\n",
        "# Best models\n",
        "best_classifier_name = classification_comparison.iloc[0]['Model']\n",
        "best_classifier = classification_results[best_classifier_name]['model']\n",
        "best_regressor_name = regression_comparison.iloc[0]['Model']\n",
        "best_regressor = regression_results[best_regressor_name]['model']\n",
        "\n",
        "print(f\"\\n🏆 Best Classification Model: {best_classifier_name}\")\n",
        "print(f\"   AUC: {classification_results[best_classifier_name]['test_auc']:.4f}\")\n",
        "print(f\"   Accuracy: {classification_results[best_classifier_name]['test_accuracy']:.4f}\")\n",
        "print(f\"   F1-Score: {classification_results[best_classifier_name]['test_f1']:.4f}\")\n",
        "\n",
        "print(f\"\\n🏆 Best Regression Model: {best_regressor_name}\")\n",
        "print(f\"   R²: {regression_results[best_regressor_name]['test_r2']:.4f}\")\n",
        "print(f\"   MAE: {regression_results[best_regressor_name]['test_mae']:.4f}\")\n",
        "print(f\"   RMSE: {regression_results[best_regressor_name]['test_rmse']:.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create comprehensive visualization of model performance\n",
        "fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "\n",
        "# 1. Classification model comparison\n",
        "models = list(classification_results.keys())\n",
        "aucs = [classification_results[name]['test_auc'] for name in models]\n",
        "accuracies = [classification_results[name]['test_accuracy'] for name in models]\n",
        "\n",
        "x = np.arange(len(models))\n",
        "width = 0.35\n",
        "\n",
        "axes[0,0].bar(x - width/2, aucs, width, label='AUC', alpha=0.8)\n",
        "axes[0,0].bar(x + width/2, accuracies, width, label='Accuracy', alpha=0.8)\n",
        "axes[0,0].set_xlabel('Models')\n",
        "axes[0,0].set_ylabel('Score')\n",
        "axes[0,0].set_title('Classification Model Performance')\n",
        "axes[0,0].set_xticks(x)\n",
        "axes[0,0].set_xticklabels(models, rotation=45, ha='right')\n",
        "axes[0,0].legend()\n",
        "axes[0,0].grid(True, alpha=0.3)\n",
        "\n",
        "# 2. Regression model comparison\n",
        "reg_models = list(regression_results.keys())\n",
        "r2_scores = [regression_results[name]['test_r2'] for name in reg_models]\n",
        "\n",
        "axes[0,1].bar(reg_models, r2_scores, color='green', alpha=0.7)\n",
        "axes[0,1].set_ylabel('R² Score')\n",
        "axes[0,1].set_title('Regression Model Performance (R²)')\n",
        "axes[0,1].tick_params(axis='x', rotation=45)\n",
        "axes[0,1].grid(True, alpha=0.3)\n",
        "\n",
        "# 3. ROC Curve for best classifier\n",
        "best_predictions_proba = classification_results[best_classifier_name]['predictions_proba']\n",
        "fpr, tpr, _ = roc_curve(y_test, best_predictions_proba)\n",
        "auc_score = classification_results[best_classifier_name]['test_auc']\n",
        "\n",
        "axes[0,2].plot(fpr, tpr, label=f'{best_classifier_name} (AUC = {auc_score:.3f})', linewidth=2)\n",
        "axes[0,2].plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
        "axes[0,2].set_xlabel('False Positive Rate')\n",
        "axes[0,2].set_ylabel('True Positive Rate')\n",
        "axes[0,2].set_title('ROC Curve - Best Classifier')\n",
        "axes[0,2].legend()\n",
        "axes[0,2].grid(True, alpha=0.3)\n",
        "\n",
        "# 4. Confusion Matrix for best classifier\n",
        "best_predictions = classification_results[best_classifier_name]['predictions']\n",
        "cm = confusion_matrix(y_test, best_predictions)\n",
        "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1,0])\n",
        "axes[1,0].set_title(f'Confusion Matrix - {best_classifier_name}')\n",
        "axes[1,0].set_ylabel('True Label')\n",
        "axes[1,0].set_xlabel('Predicted Label')\n",
        "\n",
        "# 5. Regression predictions vs actual\n",
        "best_reg_predictions = regression_results[best_regressor_name]['predictions']\n",
        "axes[1,1].scatter(y_reg_test, best_reg_predictions, alpha=0.5, s=1)\n",
        "axes[1,1].plot([y_reg_test.min(), y_reg_test.max()], [y_reg_test.min(), y_reg_test.max()], 'r--', lw=2)\n",
        "axes[1,1].set_xlabel('Actual Casualties')\n",
        "axes[1,1].set_ylabel('Predicted Casualties')\n",
        "axes[1,1].set_title(f'Predictions vs Actual - {best_regressor_name}')\n",
        "axes[1,1].grid(True, alpha=0.3)\n",
        "\n",
        "# 6. Feature importance (if available)\n",
        "if hasattr(best_classifier, 'feature_importances_'):\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': available_features,\n",
        "        'importance': best_classifier.feature_importances_\n",
        "    }).sort_values('importance', ascending=False).head(10)\n",
        "    \n",
        "    axes[1,2].barh(range(len(feature_importance)), feature_importance['importance'])\n",
        "    axes[1,2].set_yticks(range(len(feature_importance)))\n",
        "    axes[1,2].set_yticklabels(feature_importance['feature'])\n",
        "    axes[1,2].set_xlabel('Feature Importance')\n",
        "    axes[1,2].set_title(f'Top 10 Feature Importances - {best_classifier_name}')\n",
        "    axes[1,2].invert_yaxis()\n",
        "else:\n",
        "    axes[1,2].text(0.5, 0.5, 'Feature importance\\nnot available\\nfor this model', \n",
        "                   ha='center', va='center', transform=axes[1,2].transAxes)\n",
        "    axes[1,2].set_title('Feature Importance')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "# Detailed classification report for best classifier\n",
        "print(f\"\\n📋 Detailed Classification Report - {best_classifier_name}:\")\n",
        "print(classification_report(y_test, best_predictions, target_names=['Unsuccessful', 'Successful']))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Business Insights and Risk Assessment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Comprehensive business insights and risk assessment\n",
        "print(\"=== BUSINESS INSIGHTS AND RISK ASSESSMENT ===\")\n",
        "\n",
        "# 1. Predictive insights using best models\n",
        "print(\"\\n🎯 PREDICTIVE INSIGHTS:\")\n",
        "\n",
        "# Success prediction insights\n",
        "success_proba = classification_results[best_classifier_name]['predictions_proba']\n",
        "high_risk_threshold = 0.8\n",
        "high_risk_attacks = (success_proba >= high_risk_threshold).sum()\n",
        "print(f\"   High-risk attacks (success probability ≥ {high_risk_threshold}): {high_risk_attacks} ({high_risk_attacks/len(success_proba)*100:.1f}%)\")\n",
        "\n",
        "# Casualty prediction insights\n",
        "casualty_predictions = regression_results[best_regressor_name]['predictions']\n",
        "high_casualty_predicted = (casualty_predictions >= 10).sum()\n",
        "print(f\"   Predicted high-casualty events (≥10 casualties): {high_casualty_predicted} ({high_casualty_predicted/len(casualty_predictions)*100:.1f}%)\")\n",
        "\n",
        "# 2. Risk factors analysis\n",
        "print(\"\\n🚨 TOP RISK FACTORS:\")\n",
        "\n",
        "if hasattr(best_classifier, 'feature_importances_'):\n",
        "    feature_importance = pd.DataFrame({\n",
        "        'feature': available_features,\n",
        "        'importance': best_classifier.feature_importances_\n",
        "    }).sort_values('importance', ascending=False)\n",
        "    \n",
        "    print(\"   Top 10 factors for attack success prediction:\")\n",
        "    for i, (_, row) in enumerate(feature_importance.head(10).iterrows()):\n",
        "        print(f\"   {i+1:2d}. {row['feature']:25s}: {row['importance']:.4f}\")\n",
        "\n",
        "# 3. Geographical risk assessment\n",
        "print(\"\\n🌍 GEOGRAPHICAL RISK ANALYSIS:\")\n",
        "\n",
        "# Calculate risk scores by country\n",
        "country_risk = df_processed.groupby('country_txt').agg({\n",
        "    'eventid': 'count',\n",
        "    'total_casualties': ['sum', 'mean'],\n",
        "    'success_binary': 'mean'\n",
        "}).round(3)\n",
        "\n",
        "country_risk.columns = ['incidents', 'total_casualties', 'avg_casualties', 'success_rate']\n",
        "country_risk = country_risk[country_risk['incidents'] >= 100]  # Filter for significant data\n",
        "\n",
        "# Create composite risk score\n",
        "country_risk['risk_score'] = (\n",
        "    country_risk['incidents'].rank(pct=True) * 0.3 +\n",
        "    country_risk['avg_casualties'].rank(pct=True) * 0.4 +\n",
        "    country_risk['success_rate'].rank(pct=True) * 0.3\n",
        ")\n",
        "\n",
        "top_risk_countries = country_risk.sort_values('risk_score', ascending=False).head(10)\n",
        "print(\"   Top 10 highest-risk countries:\")\n",
        "for country, stats in top_risk_countries.iterrows():\n",
        "    print(f\"   {country:20s}: Risk={stats['risk_score']:.3f}, Incidents={stats['incidents']:4.0f}, Avg Casualties={stats['avg_casualties']:5.2f}\")\n",
        "\n",
        "# 4. Temporal risk patterns\n",
        "print(\"\\n📅 TEMPORAL RISK PATTERNS:\")\n",
        "\n",
        "# Monthly risk analysis\n",
        "monthly_risk = df_processed.groupby('imonth').agg({\n",
        "    'eventid': 'count',\n",
        "    'total_casualties': 'mean',\n",
        "    'success_binary': 'mean'\n",
        "}).round(3)\n",
        "\n",
        "highest_risk_month = monthly_risk['success_binary'].idxmax()\n",
        "highest_casualty_month = monthly_risk['total_casualties'].idxmax()\n",
        "most_active_month = monthly_risk['eventid'].idxmax()\n",
        "\n",
        "print(f\"   Highest success rate: {calendar.month_name[int(highest_risk_month)]} ({monthly_risk.loc[highest_risk_month, 'success_binary']:.3f})\")\n",
        "print(f\"   Highest avg casualties: {calendar.month_name[int(highest_casualty_month)]} ({monthly_risk.loc[highest_casualty_month, 'total_casualties']:.2f})\")\n",
        "print(f\"   Most active month: {calendar.month_name[int(most_active_month)]} ({monthly_risk.loc[most_active_month, 'eventid']:,} incidents)\")\n",
        "\n",
        "# 5. Attack method risk analysis\n",
        "print(\"\\n💥 ATTACK METHOD RISK ANALYSIS:\")\n",
        "\n",
        "method_risk = df_processed.groupby('attacktype1_txt').agg({\n",
        "    'eventid': 'count',\n",
        "    'total_casualties': 'mean',\n",
        "    'success_binary': 'mean'\n",
        "}).round(3)\n",
        "\n",
        "method_risk = method_risk[method_risk['eventid'] >= 100]  # Filter for significant data\n",
        "method_risk = method_risk.sort_values('total_casualties', ascending=False)\n",
        "\n",
        "print(\"   Most lethal attack methods (avg casualties):\")\n",
        "for method, stats in method_risk.head(8).iterrows():\n",
        "    print(f\"   {method:25s}: {stats['total_casualties']:5.2f} casualties, {stats['success_binary']:.3f} success rate\")\n",
        "\n",
        "# 6. Recommendations\n",
        "print(\"\\n💡 STRATEGIC RECOMMENDATIONS:\")\n",
        "print(\"   1. PREVENTION PRIORITIES:\")\n",
        "print(\"      🔸 Focus on high-risk countries and regions\")\n",
        "print(\"      🔸 Enhanced monitoring during high-risk months\")\n",
        "print(\"      🔸 Targeted countermeasures for most lethal attack types\")\n",
        "print(\"      🔸 Early warning systems based on predictive models\")\n",
        "\n",
        "print(\"   2. RESOURCE ALLOCATION:\")\n",
        "print(\"      🔸 Prioritize security in countries with highest risk scores\")\n",
        "print(\"      🔸 Increase preparedness for high-casualty attack methods\")\n",
        "print(\"      🔸 Deploy predictive models for real-time threat assessment\")\n",
        "print(\"      🔸 Focus on preventing attacks with high success probability\")\n",
        "\n",
        "print(\"   3. MONITORING AND INTELLIGENCE:\")\n",
        "print(\"      🔸 Enhanced surveillance during peak risk periods\")\n",
        "print(\"      🔸 Pattern recognition for emerging threat vectors\")\n",
        "print(\"      🔸 International cooperation in high-risk regions\")\n",
        "print(\"      🔸 Continuous model updating with new incident data\")\n",
        "\n",
        "# 7. Model deployment insights\n",
        "print(\"\\n🎯 MODEL DEPLOYMENT INSIGHTS:\")\n",
        "print(f\"   Success Prediction Model ({best_classifier_name}):\")\n",
        "print(f\"      - Accuracy: {classification_results[best_classifier_name]['test_accuracy']:.1%}\")\n",
        "print(f\"      - AUC Score: {classification_results[best_classifier_name]['test_auc']:.3f}\")\n",
        "print(f\"      - Use case: Early warning system for attack prevention\")\n",
        "\n",
        "print(f\"   Casualty Prediction Model ({best_regressor_name}):\")\n",
        "print(f\"      - R² Score: {regression_results[best_regressor_name]['test_r2']:.3f}\")\n",
        "print(f\"      - RMSE: {regression_results[best_regressor_name]['test_rmse']:.2f} casualties\")\n",
        "print(f\"      - Use case: Emergency response planning and resource allocation\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "print(\"GLOBAL TERRORISM ANALYSIS COMPLETED SUCCESSFULLY!\")\n",
        "print(\"This analysis provides actionable insights for security and policy decision-making.\")\n",
        "print(\"=\"*80)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}